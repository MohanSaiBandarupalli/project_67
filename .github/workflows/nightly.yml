name: Nightly Full Pipeline

on:
  schedule:
    # Runs every day at 03:30 UTC (adjust if you want)
    - cron: "30 3 * * *"
  workflow_dispatch: {}

jobs:
  full-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Poetry (pinned)
        run: |
          python -m pip install --upgrade pip
          pip install "poetry==2.0.0"

      - name: Configure Poetry (project venv)
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true

      - name: Cache Poetry cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pypoetry
            ~/.cache/pip
          key: ${{ runner.os }}-poetry-cache-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            ${{ runner.os }}-poetry-cache-

      - name: Cache .venv
        uses: actions/cache@v4
        with:
          path: .venv
          key: ${{ runner.os }}-venv-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            ${{ runner.os }}-venv-

      - name: Install dependencies
        run: |
          poetry install --no-interaction --no-ansi --with dev

      - name: Show environment info
        run: |
          poetry --version
          poetry run python -V
          poetry run python -c "import sys; print(sys.executable)"

      #  Create tiny synthetic MovieLens-like dataset at the EXACT path your pipeline expects
      - name: Create tiny MovieLens dataset (CI)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data/external/movielens/ml-32m

          python - <<'PY'
          import csv, random, time, os
          from pathlib import Path

          base = Path("data/external/movielens/ml-32m")
          base.mkdir(parents=True, exist_ok=True)

          # Generate a small but non-trivial dataset: enough users/items for graphs + ranking + churn smoke
          random.seed(42)

          n_users = 400
          n_items = 800
          n_rows  = 25000

          # ratings.csv: userId,movieId,rating,timestamp
          ratings_path = base / "ratings.csv"
          with ratings_path.open("w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["userId","movieId","rating","timestamp"])
            ts0 = 1577836800  # 2020-01-01
            for i in range(n_rows):
              u = random.randint(1, n_users)
              m = random.randint(1, n_items)
              r = random.choice([2.5,3.0,3.5,4.0,4.5,5.0])
              ts = ts0 + random.randint(0, 60*60*24*120)
              w.writerow([u, m, r, ts])

          # movies.csv: movieId,title,genres
          movies_path = base / "movies.csv"
          genres = ["Action","Comedy","Drama","Thriller","Romance","Sci-Fi","Adventure","Animation"]
          with movies_path.open("w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["movieId","title","genres"])
            for m in range(1, n_items+1):
              g = "|".join(random.sample(genres, k=random.randint(1,3)))
              w.writerow([m, f"Movie {m}", g])

          # tags.csv: userId,movieId,tag,timestamp (optional but nice)
          tags_path = base / "tags.csv"
          tags = ["funny","dark","slow","classic","underrated","cult","family","epic"]
          with tags_path.open("w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["userId","movieId","tag","timestamp"])
            for _ in range(2000):
              u = random.randint(1, n_users)
              m = random.randint(1, n_items)
              t = random.choice(tags)
              ts = ts0 + random.randint(0, 60*60*24*120)
              w.writerow([u, m, t, ts])

          # links.csv (optional)
          links_path = base / "links.csv"
          with links_path.open("w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["movieId","imdbId","tmdbId"])
            for m in range(1, n_items+1):
              w.writerow([m, str(100000+m), str(200000+m)])

          print("Wrote:", ratings_path, movies_path, tags_path, links_path)
          PY

      #  Full pipeline execution (fail if a stage fails)
      - name: Run NTG pipeline end-to-end
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data/processed/splits data/features outputs/graph outputs/recommendations outputs/risk reports data/interim/duckdb_tmp

          # Day-1
          poetry run python -m ntg.pipelines.build_dataset_duckdb

          # Day-2 (if you have separate modules, replace accordingly)
          if poetry run python -c "import importlib; importlib.import_module('ntg.pipelines.build_features')" >/dev/null 2>&1; then
            poetry run python -m ntg.pipelines.build_features
          fi

          # Day-3
          poetry run python -m ntg.graph.build_graph

          # Day-4
          poetry run python -m ntg.models.ranker

          # Day-6 (churn / risk)
          if poetry run python -c "import importlib; importlib.import_module('ntg.churn.score_users')" >/dev/null 2>&1; then
            poetry run python -m ntg.churn.score_users
          fi

      #  Run tests after artifacts exist (so pipeline contract tests pass)
      - name: Run tests (unit only)
        run: |
          poetry run pytest -q -m "not integration"

      - name: Upload artifacts (debug)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ntg-nightly-artifacts
          path: |
            outputs/**
            reports/**
            data/features/**
            data/processed/splits/**
          if-no-files-found: warn

      - name: Build Docker image (nightly)
        run: |
          docker build -t ntg:nightly .
